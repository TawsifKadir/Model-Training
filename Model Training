{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13187378,"sourceType":"datasetVersion","datasetId":8126229},{"sourceId":13219918,"sourceType":"datasetVersion","datasetId":8365931}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport gc\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.utils import to_categorical","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:10:34.308850Z","iopub.execute_input":"2025-10-01T18:10:34.309025Z","iopub.status.idle":"2025-10-01T18:10:53.655103Z","shell.execute_reply.started":"2025-10-01T18:10:34.309008Z","shell.execute_reply":"2025-10-01T18:10:53.654505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Color Maps for Tissue and Nuclei","metadata":{}},{"cell_type":"code","source":"unique_nuclei_labels = [\n    \"background\",\n    \"nuclei_apoptosis\",\n    \"nuclei_endothelium\",\n    \"nuclei_epithelium\",\n    \"nuclei_histiocyte\",\n    \"nuclei_lymphocyte\",\n    \"nuclei_melanophage\",\n    \"nuclei_neutrophil\",\n    \"nuclei_plasma_cell\",\n    \"nuclei_stroma\",\n    \"nuclei_tumor\"\n]\nnuclei_class_id_map = {name: idx for idx, name in enumerate(unique_nuclei_labels)}\n\ntissue_class_id_map = {\n    \"tissue_white_background\": 0,\n    \"tissue_stroma\": 1,\n    \"tissue_blood_vessel\": 2,\n    \"tissue_tumor\": 3,\n    \"tissue_epidermis\": 4,\n    \"tissue_necrosis\": 5\n}\n\n# Colors\nnuclei_colormap = {\n    \"background\": (0.0, 0.0, 0.0),\n    \"nuclei_apoptosis\": (1.0, 0.0, 0.0),\n    \"nuclei_endothelium\": (0.0, 1.0, 0.0),\n    \"nuclei_epithelium\": (0.0, 0.0, 1.0),\n    \"nuclei_histiocyte\": (1.0, 1.0, 0.0),\n    \"nuclei_lymphocyte\": (0.0, 1.0, 1.0),\n    \"nuclei_melanophage\": (1.0, 0.0, 1.0),\n    \"nuclei_neutrophil\": (0.5, 0.5, 0.0),\n    \"nuclei_plasma_cell\": (0.5, 0.0, 0.5),\n    \"nuclei_stroma\": (0.0, 0.5, 0.5),\n    \"nuclei_tumor\": (0.3, 0.3, 0.3),\n}\n\nnuclei_rgb_colors = [\n    (0,0,0),\n    (0,0,255),\n    (0,127,127),\n    (0,255,0),\n    (0,255,255),\n    (76,76,76),\n    (127,0,127),\n    (127,127,0),\n    (255,0,0),\n    (255,0,255),\n    (255,255,0)\n]\n\ntissue_colormap = {\n    \"tissue_white_background\": (1.0, 1.0, 1.0),\n    \"tissue_stroma\": (0.7, 0.7, 0.7),\n    \"tissue_blood_vessel\": (1.0, 0.6, 0.0),\n    \"tissue_tumor\": (0.8, 0.1, 0.1),\n    \"tissue_epidermis\": (0.1, 0.8, 0.1),\n    \"tissue_necrosis\": (0.5, 0.2, 0.2),\n}\n\ntissue_rgb_colors = [\n    (25,204,25),\n    (127,51,51),\n    (178,178,178),\n    (204,25,25),\n    (255,153,0),\n    (255,255,255)\n]\n\nnuclei_color_to_class = {rgb: idx for idx, rgb in enumerate(nuclei_rgb_colors)}\ntissue_color_to_class = {\n    (25,204,25): 0,\n    (127, 51, 51): 1,      # tissue_stroma\n    (204, 25, 25): 3,      # tissue_tumor\n    (178, 178, 178): 2,    # tissue_blood_vessel\n    (255, 153, 0): 4,      # tissue_epidermis\n    (255, 255, 255): 0     # tissue_white_background\n}\n\n# ---- Mapper Functions ----\n# def create_color_to_class_map(colormap, class_id_map):\n#     \"\"\"Convert colormap dict to a {rgb_tuple: class_id} lookup.\"\"\"\n#     return {tuple(np.array(color) * 255): class_id_map[name]\n#             for name, color in colormap.items()}\n\n\n# nuclei_color_to_class = create_color_to_class_map(nuclei_colormap, nuclei_class_id_map)\n# tissue_color_to_class = create_color_to_class_map(tissue_colormap, tissue_class_id_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:10:53.669162Z","iopub.execute_input":"2025-10-01T18:10:53.669419Z","iopub.status.idle":"2025-10-01T18:10:53.707956Z","shell.execute_reply.started":"2025-10-01T18:10:53.669396Z","shell.execute_reply":"2025-10-01T18:10:53.707432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load dataset into arrays","metadata":{}},{"cell_type":"code","source":"# base_path = '/kaggle/input/puma-training/puma-training/PUMA'\nbase_path = '/kaggle/input/puma-training-2/puma-training-2'\n\n# Define full folder paths\nnuclei_geojson_path = os.path.join(base_path, '01_training_dataset_geojson_nuclei')\ntissue_geojson_path = os.path.join(base_path, '01_training_dataset_geojson_tissue', '01_training_dataset_geojson_tissue')\n# roi_tif_path        = os.path.join(base_path, '01_training_dataset_tif_ROIs', '01_training_dataset_tif_ROIs')\n# nuclei_mask_path    = os.path.join(base_path, 'nuclei_masks')\n# tissue_mask_path    = os.path.join(base_path, 'tissue_masks')\nroi_tif_path        = os.path.join(base_path, 'images')\nnuclei_mask_path    = os.path.join(base_path, 'nuclei_masks')\ntissue_mask_path    = os.path.join(base_path, 'tissue_masks')\n# Load all files into sorted arrays (aligned indices)\n# nuclei_files           = sorted([os.path.join(nuclei_geojson_path, f) for f in os.listdir(nuclei_geojson_path) if f.endswith('.geojson')])\n# tissue_files           = sorted([os.path.join(tissue_geojson_path, f) for f in os.listdir(tissue_geojson_path) if f.endswith('.geojson')])\nroi_tif_files          = sorted([os.path.join(roi_tif_path, f) for f in os.listdir(roi_tif_path) if f.endswith('.png')])\nnuclei_mask_png_files  = sorted([os.path.join(nuclei_mask_path, f) for f in os.listdir(nuclei_mask_path) if f.endswith('.png')])\ntissue_mask_png_files  = sorted([os.path.join(tissue_mask_path, f) for f in os.listdir(tissue_mask_path) if f.endswith('.png')])\n\n# print(\"🧠 Nuclei:\", nuclei_files[0])\n# print(\"🧫 Tissue:\", tissue_files[0])\nprint(\"🖼️  ROI TIF:\", roi_tif_files[0])\nprint(\"🌍 Nuclei Mask:\", nuclei_mask_png_files[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:00.741121Z","iopub.execute_input":"2025-10-01T18:11:00.741449Z","iopub.status.idle":"2025-10-01T18:11:01.025787Z","shell.execute_reply.started":"2025-10-01T18:11:00.741426Z","shell.execute_reply":"2025-10-01T18:11:01.025138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dice Coefficient and Dice Loss","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import backend as K\nimport tensorflow as tf\n\nIMG_SIZE = 512\nEPOCHS = 100\nBATCH_SIZE = 8\nLR = 1e-3  # learning rate\nSMOOTH = 1e-6\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef dice_for_class(class_id, num_classes, name=None):\n    \"\"\"\n    Returns a Dice metric function for a specific class index.\n    \"\"\"\n    def dice(y_true, y_pred):\n        y_true_c = K.cast(K.equal(K.argmax(y_true, axis=-1), class_id), \"float32\")\n        y_pred_c = K.cast(K.equal(K.argmax(y_pred, axis=-1), class_id), \"float32\")\n        intersection = K.sum(y_true_c * y_pred_c)\n        return (2. * intersection + 1e-7) / (K.sum(y_true_c) + K.sum(y_pred_c) + 1e-7)\n    return tf.keras.metrics.MeanMetricWrapper(dice, name=name or f\"dice_class_{class_id}\")\n\ndef iou_for_class(class_id, num_classes, name=None):\n    \"\"\"\n    Returns an IoU metric function for a specific class index.\n    \"\"\"\n    def iou(y_true, y_pred):\n        y_true_c = K.cast(K.equal(K.argmax(y_true, axis=-1), class_id), \"float32\")\n        y_pred_c = K.cast(K.argmax(y_pred, axis=-1), class_id), \"float32\")\n        intersection = K.sum(y_true_c * y_pred_c)\n        union = K.sum(y_true_c) + K.sum(y_pred_c) - intersection\n        return (intersection + 1e-7) / (union + 1e-7)\n    return tf.keras.metrics.MeanMetricWrapper(iou, name=name or f\"iou_class_{class_id}\")\n\n\ndef iou_coef(y_true, y_pred, smooth=SMOOTH):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred > 0.5, tf.float32)  # thresholded\n    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n    union = tf.reduce_sum(y_true + y_pred, axis=[1,2,3]) - intersection\n    iou = tf.reduce_mean((intersection + smooth) / (union + smooth))\n    return iou\n\n\ndef dice_coef(y_true, y_pred, smooth=SMOOTH):\n    \"\"\"\n    Computes Dice coefficient for multi-class segmentation\n    y_true: one-hot encoded ground truth\n    y_pred: softmax output\n    \"\"\"\n    y_true_f = tf.reshape(y_true, [-1])\n    y_pred_f = tf.reshape(y_pred, [-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2.0 * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\ndef weighted_dice_coef(y_true, y_pred, class_weights=None, smooth=SMOOTH):\n    \"\"\"Weighted Dice coefficient.\"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n\n    # Flatten to (N, C)\n    y_true_f = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n    y_pred_f = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n    denominator = tf.reduce_sum(y_true_f + y_pred_f, axis=0)\n\n    dice_per_class = (2. * intersection + smooth) / (denominator + smooth)\n\n    if class_weights is not None:\n        class_weights = tf.convert_to_tensor(class_weights, dtype=tf.float32)\n        dice_per_class = dice_per_class * class_weights\n\n    # Weighted mean\n    return tf.reduce_sum(dice_per_class) / (\n        tf.reduce_sum(class_weights) if class_weights is not None else tf.cast(tf.shape(dice_per_class)[0], tf.float32)\n    )\n\ndef weighted_dice_loss(y_true, y_pred, class_weights=None):\n    \"\"\"1 - Weighted Dice coefficient\"\"\"\n    return 1.0 - weighted_dice_coef(y_true, y_pred, class_weights)\n\ndef weighted_focal_dice_loss(y_true, y_pred, class_weights=None, gamma=2.0):\n    \"\"\"Focal Dice loss with weights.\"\"\"\n    dice = weighted_dice_coef(y_true, y_pred, class_weights)\n    return tf.pow((1.0 - dice), gamma)\n\ndef combined_weighted_dice_loss(y_true, y_pred, class_weights=None, alpha=0.5, gamma=2.0):\n    \"\"\"Blend weighted dice loss + focal dice loss.\"\"\"\n    wdl = weighted_dice_loss(y_true, y_pred, class_weights)\n    fdl = weighted_focal_dice_loss(y_true, y_pred, class_weights, gamma)\n    return alpha * wdl + (1.0 - alpha) * fdl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:08.273602Z","iopub.execute_input":"2025-10-01T18:11:08.273898Z","iopub.status.idle":"2025-10-01T18:11:08.283670Z","shell.execute_reply.started":"2025-10-01T18:11:08.273876Z","shell.execute_reply":"2025-10-01T18:11:08.283134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Helper Blocks for different models","metadata":{}},{"cell_type":"code","source":"# -------------------- Helper Blocks --------------------\ndef conv_block(x, filters):\n    x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n    x = layers.BatchNormalization()(x)\n    return x\n\n# ---- Attention U-Net ----\ndef attention_block(x, g, filters):\n    \"\"\"\n    Attention Block for Attention U-Net\n    x: skip connection tensor\n    g: gating signal tensor (from deeper layer)\n    filters: number of filters for intermediate conv layers\n    \"\"\"\n    # Match shapes\n    theta_x = layers.Conv2D(filters, 1, strides=1, padding='same')(x)   # (H, W, filters)\n    phi_g   = layers.Conv2D(filters, 1, strides=1, padding='same')(g)   # (H', W', filters)\n\n    # Resize gating to match skip connection\n    if theta_x.shape[1] != phi_g.shape[1] or theta_x.shape[2] != phi_g.shape[2]:\n        phi_g = layers.UpSampling2D(size=(\n            theta_x.shape[1] // phi_g.shape[1],\n            theta_x.shape[2] // phi_g.shape[2]\n        ))(phi_g)\n\n    add     = layers.Add()([theta_x, phi_g])\n    act     = layers.Activation('relu')(add)\n    psi     = layers.Conv2D(1, 1, strides=1, padding='same')(act)\n    psi     = layers.Activation('sigmoid')(psi)\n\n    # Apply attention\n    return layers.Multiply()([x, psi])\n\n\n# ---- Sharp U-Net ----\ndef sharp_block(x, filters):\n    edge = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n    edge = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(edge)\n    return edge\n\n# ---- MultiRes U-Net ----\ndef multires_block(x, filters):\n    c1 = layers.Conv2D(filters//6, 3, padding=\"same\", activation=\"relu\")(x)\n    c2 = layers.Conv2D(filters//3, 3, padding=\"same\", activation=\"relu\")(c1)\n    c3 = layers.Conv2D(filters//2, 3, padding=\"same\", activation=\"relu\")(c2)\n    out = layers.concatenate([c1, c2, c3], axis=-1)\n    out = layers.BatchNormalization()(out)\n    return out\n\ndef respath(x, filters, length=3):\n    for _ in range(length):\n        shortcut = layers.Conv2D(filters, 1, padding=\"same\")(x)\n        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Add()([x, shortcut])\n    return x\n\n# -------------------- Model Variants --------------------\ndef build_unet(input_shape, num_classes):\n    inputs = layers.Input(input_shape)\n    c1 = conv_block(inputs, 64); p1 = layers.MaxPooling2D()(c1)\n    c2 = conv_block(p1, 128); p2 = layers.MaxPooling2D()(c2)\n    c3 = conv_block(p2, 256); p3 = layers.MaxPooling2D()(c3)\n    c4 = conv_block(p3, 512); p4 = layers.MaxPooling2D()(c4)\n    b  = conv_block(p4, 1024)\n\n    u4 = layers.UpSampling2D()(b); u4 = layers.concatenate([u4, c4]); d4 = conv_block(u4, 512)\n    u3 = layers.UpSampling2D()(d4); u3 = layers.concatenate([u3, c3]); d3 = conv_block(u3, 256)\n    u2 = layers.UpSampling2D()(d3); u2 = layers.concatenate([u2, c2]); d2 = conv_block(u2, 128)\n    u1 = layers.UpSampling2D()(d2); u1 = layers.concatenate([u1, c1]); d1 = conv_block(u1, 64)\n\n    outputs = layers.Conv2D(num_classes, 1, activation=\"softmax\")(d1)\n    return models.Model(inputs, outputs, name=\"U-Net\")\n\ndef build_attention_unet(input_shape, num_classes):\n    inputs = layers.Input(input_shape)\n    c1 = conv_block(inputs, 64); p1 = layers.MaxPooling2D()(c1)\n    c2 = conv_block(p1, 128); p2 = layers.MaxPooling2D()(c2)\n    c3 = conv_block(p2, 256); p3 = layers.MaxPooling2D()(c3)\n    c4 = conv_block(p3, 512); p4 = layers.MaxPooling2D()(c4)\n    b  = conv_block(p4, 1024)\n\n    g4 = layers.Conv2D(512, 1)(b); att4 = attention_block(c4, g4, 512)\n    u4 = layers.UpSampling2D()(b); u4 = layers.concatenate([u4, att4]); d4 = conv_block(u4, 512)\n\n    g3 = layers.Conv2D(256, 1)(d4); att3 = attention_block(c3, g3, 256)\n    u3 = layers.UpSampling2D()(d4); u3 = layers.concatenate([u3, att3]); d3 = conv_block(u3, 256)\n\n    g2 = layers.Conv2D(128, 1)(d3); att2 = attention_block(c2, g2, 128)\n    u2 = layers.UpSampling2D()(d3); u2 = layers.concatenate([u2, att2]); d2 = conv_block(u2, 128)\n\n    g1 = layers.Conv2D(64, 1)(d2); att1 = attention_block(c1, g1, 64)\n    u1 = layers.UpSampling2D()(d2); u1 = layers.concatenate([u1, att1]); d1 = conv_block(u1, 64)\n\n    outputs = layers.Conv2D(num_classes, 1, activation=\"softmax\")(d1)\n    return models.Model(inputs, outputs, name=\"Attention_U-Net\")\n\ndef build_sharp_unet(input_shape, num_classes):\n    inputs = layers.Input(input_shape)\n    c1 = conv_block(inputs, 64); p1 = layers.MaxPooling2D()(c1)\n    c2 = conv_block(p1, 128); p2 = layers.MaxPooling2D()(c2)\n    c3 = conv_block(p2, 256); p3 = layers.MaxPooling2D()(c3)\n    c4 = conv_block(p3, 512); p4 = layers.MaxPooling2D()(c4)\n    b  = conv_block(p4, 1024)\n\n    u4 = layers.UpSampling2D()(b); s4 = sharp_block(c4, 512); u4 = layers.concatenate([u4, s4]); d4 = conv_block(u4, 512)\n    u3 = layers.UpSampling2D()(d4); s3 = sharp_block(c3, 256); u3 = layers.concatenate([u3, s3]); d3 = conv_block(u3, 256)\n    u2 = layers.UpSampling2D()(d3); s2 = sharp_block(c2, 128); u2 = layers.concatenate([u2, s2]); d2 = conv_block(u2, 128)\n    u1 = layers.UpSampling2D()(d2); s1 = sharp_block(c1, 64);  u1 = layers.concatenate([u1, s1]); d1 = conv_block(u1, 64)\n\n    outputs = layers.Conv2D(num_classes, 1, activation=\"softmax\")(d1)\n    return models.Model(inputs, outputs, name=\"Sharp_U-Net\")\n\ndef build_multires_unet(input_shape, num_classes):\n    inputs = layers.Input(input_shape)\n    m1 = multires_block(inputs, 64); p1 = layers.MaxPooling2D()(m1); r1 = respath(m1, 64)\n    m2 = multires_block(p1, 128); p2 = layers.MaxPooling2D()(m2); r2 = respath(m2, 128)\n    m3 = multires_block(p2, 256); p3 = layers.MaxPooling2D()(m3); r3 = respath(m3, 256)\n    m4 = multires_block(p3, 512); p4 = layers.MaxPooling2D()(m4); r4 = respath(m4, 512)\n    b  = multires_block(p4, 1024)\n\n    u4 = layers.UpSampling2D()(b); u4 = layers.concatenate([u4, r4]); m5 = multires_block(u4, 512)\n    u3 = layers.UpSampling2D()(m5); u3 = layers.concatenate([u3, r3]); m6 = multires_block(u3, 256)\n    u2 = layers.UpSampling2D()(m6); u2 = layers.concatenate([u2, r2]); m7 = multires_block(u2, 128)\n    u1 = layers.UpSampling2D()(m7); u1 = layers.concatenate([u1, r1]); m8 = multires_block(u1, 64)\n\n    outputs = layers.Conv2D(num_classes, 1, activation=\"softmax\")(m8)\n    return models.Model(inputs, outputs, name=\"MultiRes_U-Net\")\n\n# -------------------- Unified Builder --------------------\ndef build_segmentation_model(model_type=\"unet\", input_shape=(256,256,3), num_classes=6):\n    if model_type == \"unet\":\n        return build_unet(input_shape, num_classes)\n    elif model_type == \"attention\":\n        return build_attention_unet(input_shape, num_classes)\n    elif model_type == \"sharp\":\n        return build_sharp_unet(input_shape, num_classes)\n    elif model_type == \"multires\":\n        return build_multires_unet(input_shape, num_classes)\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:12.047648Z","iopub.execute_input":"2025-10-01T18:11:12.047922Z","iopub.status.idle":"2025-10-01T18:11:12.070491Z","shell.execute_reply.started":"2025-10-01T18:11:12.047900Z","shell.execute_reply":"2025-10-01T18:11:12.069783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_augmentation():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Rotate(limit=(15, 30), p=0.5),  \n        A.CLAHE(clip_limit=4.0, tile_grid_size=(8,8), p=0.5)\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:15.905166Z","iopub.execute_input":"2025-10-01T18:11:15.905443Z","iopub.status.idle":"2025-10-01T18:11:23.672197Z","shell.execute_reply.started":"2025-10-01T18:11:15.905422Z","shell.execute_reply":"2025-10-01T18:11:23.671594Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport tensorflow as tf\n\ndef rgb_to_class(mask_rgb, color_to_class):\n    h, w, _ = mask_rgb.shape\n    class_mask = np.zeros((h,w), dtype=np.uint8)\n    for rgb, class_id in color_to_class.items():\n        match = np.all(mask_rgb == rgb, axis=-1)\n        class_mask[match] = class_id\n    return class_mask\n\ndef mask_to_color(mask, colormap, class_id_map):\n    h, w = mask.shape\n    rgb_mask = np.zeros((h, w, 3), dtype=np.float32)\n\n    # Build reverse mapping: class ID -> RGB\n    id_to_rgb = {v: np.array(colormap[k], dtype=np.float32) for k, v in class_id_map.items()}\n\n    for class_id, rgb in id_to_rgb.items():\n        rgb_mask[mask == class_id] = rgb\n\n    return rgb_mask\n\ndef class_to_onehot(class_mask, num_classes):\n    return to_categorical(class_mask, num_classes=num_classes).astype(np.float32)\n\n# ---- Example Usage ----\ndef preprocess_mask(mask_rgb, mode):\n    # If mask already looks one-hot, skip conversion\n    # if mask_rgb.ndim == 3 and mask_rgb.shape[-1] > 3:\n    #     return mask_rgb.astype(np.uint8)\n\n    if mode == \"nuclei\":\n        class_mask = rgb_to_class(mask_rgb, nuclei_color_to_class)\n        one_hot = class_to_onehot(class_mask, len(nuclei_class_id_map))\n    elif mode == \"tissue\":\n        class_mask = rgb_to_class(mask_rgb, tissue_color_to_class)\n        one_hot = class_to_onehot(class_mask, len(tissue_class_id_map))\n    else:\n        raise ValueError(\"Invalid mode\")\n\n    return one_hot\n\n\ndef load_image(path, target_size=(IMG_SIZE, IMG_SIZE)):\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, target_size)\n    img = img.astype(np.float32) / 255.0\n    return img\n\ndef load_mask(path, target_size=(IMG_SIZE, IMG_SIZE)):\n    \"\"\"Load mask as RGB for preprocessing.\"\"\"\n    mask_rgb = cv2.imread(path, cv2.IMREAD_COLOR)\n    mask_rgb = cv2.cvtColor(mask_rgb, cv2.COLOR_BGR2RGB)\n    mask_rgb = cv2.resize(mask_rgb, target_size, interpolation=cv2.INTER_NEAREST)\n    # print(np.unique(mask_rgb.reshape(-1,3), axis=0))\n    return mask_rgb\n\n# ---- Data Generator ----\ndef data_generator(img_files, mode=\"nuclei\", mask_files=None, batch_size=BATCH_SIZE, augment=None):\n    \"\"\"\n    Flexible data generator:\n      - If mask_files is provided, use it.\n      - Otherwise, select mask files based on mode.\n    \"\"\"\n    if mask_files is None:\n        if mode == \"nuclei\":\n            mask_files = nuclei_mask_png_files\n            num_classes = len(nuclei_class_id_map)\n        elif mode == \"tissue\":\n            mask_files = tissue_mask_png_files\n            num_classes = len(tissue_class_id_map)\n        else:\n            raise ValueError(\"mode must be 'nuclei' or 'tissue'\")\n    else:\n        num_classes = len(nuclei_class_id_map) if mode == \"nuclei\" else len(tissue_class_id_map)\n\n    while True:\n        for i in range(0, len(img_files), batch_size):\n            batch_imgs = []\n            batch_masks = []\n\n            for j in range(i, min(i + batch_size, len(img_files))):\n                img = load_image(img_files[j])\n                mask_rgb = load_mask(mask_files[j])\n\n                # Augmentation\n                if augment:\n                    augmented = augment(image=img, mask=mask_rgb)\n                    img = augmented['image']\n                    mask_rgb = augmented['mask']\n\n                mask = preprocess_mask(mask_rgb, mode)\n\n                batch_imgs.append(img.astype(np.float32))\n                batch_masks.append(mask.astype(np.float32))\n\n            batch_imgs = np.array(batch_imgs, dtype=np.float32)\n            batch_masks = np.array(batch_masks, dtype=np.float32)\n            \n            yield batch_imgs, batch_masks\n\n\n\n# # Example usage\n# gen = data_generator(roi_tif_files, mode=\"nuclei\")\n# batch_imgs, batch_masks = next(gen)\n\n# print(\"First batch imgs shape:\", batch_imgs.shape)\n# print(\"First batch masks shape:\", batch_masks.shape)\n# print(\"Unique mask classes in first batch:\", np.unique(np.argmax(batch_masks, axis=-1)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:23.686506Z","iopub.execute_input":"2025-10-01T18:11:23.686701Z","iopub.status.idle":"2025-10-01T18:11:23.710605Z","shell.execute_reply.started":"2025-10-01T18:11:23.686686Z","shell.execute_reply":"2025-10-01T18:11:23.710093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Function","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass DisplayPredictionsCallback(tf.keras.callbacks.Callback):\n    def __init__(self, X_val, Y_val, mode=\"tissue\", interval=10):\n        super().__init__()\n        self.X_val = X_val\n        self.Y_val = Y_val\n        self.mode = mode\n        self.interval = interval\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.interval != 0:\n            return\n\n        y_val_np = np.array(self.Y_val)   # Convert list → numpy\n        \n        # print(\"y_val shape:\", y_val_np.shape)\n        # print(\"y_val dtype:\", y_val_np.dtype)\n        \n        # For one-hot masks\n        if y_val_np.ndim == 4:  # (N, H, W, C)\n            unique_vals = np.unique(y_val_np.reshape(-1, y_val_np.shape[-1]), axis=0)\n        else:  # For class ID masks (N, H, W)\n            unique_vals = np.unique(y_val_np)\n        \n        # print(\"Unique values in y_val:\", unique_vals[:10])\n        idx = np.random.randint(0, len(self.X_val))\n        img = np.expand_dims(self.X_val[idx], 0)\n\n        # Model prediction\n        pred = self.model.predict(img)[0]  # (H, W, C)\n        pred_mask = np.argmax(pred, axis=-1)  # (H, W)\n\n        # Prepare ground truth mask\n        true_onehot = preprocess_mask(self.Y_val[idx], self.mode)  # (H, W, C)\n        true_mask = np.argmax(true_onehot, axis=-1)  # (H, W)\n\n        if self.mode == \"nuclei\":\n            colormap = nuclei_colormap\n            class_id_map = nuclei_class_id_map\n        else:\n            colormap = tissue_colormap\n            class_id_map = tissue_class_id_map\n\n        true_rgb = mask_to_color(true_mask, colormap, class_id_map)  # CHANGED\n        pred_rgb = mask_to_color(pred_mask, colormap, class_id_map)  # CHANGED\n\n        # --- NEW: log counts per class ---\n        # unique_true, counts_true = np.unique(true_mask, return_counts=True)\n        # unique_pred, counts_pred = np.unique(pred_mask, return_counts=True)\n        # print(f\"\\n[Epoch {epoch+1}] Sample {idx}\")\n        # print(\"Ground truth class counts:\")\n        # for u, c in zip(unique_true, counts_true):\n        #     print(f\"  Class {u}: {c} pixels\")\n        # print(\"Prediction class counts:\")\n        # for u, c in zip(unique_pred, counts_pred):\n        #     print(f\"  Class {u}: {c} pixels\")\n        # -----------------------------------\n        \n        # Visualization\n        plt.figure(figsize=(15,5))\n        plt.subplot(1,3,1)\n        plt.imshow(self.X_val[idx])\n        plt.title(\"Input\")\n        plt.axis(\"off\")\n\n        plt.subplot(1,3,2)\n        plt.imshow(true_rgb)\n        plt.title(\"Ground Truth\")\n        plt.axis(\"off\")\n\n        plt.subplot(1,3,3)\n        plt.imshow(pred_rgb)\n        plt.title(\"Prediction\")\n        plt.axis(\"off\")\n        plt.show()\n\n        \ndef train_segmentation(img_files, mode=\"tissue\", model_type=\"unet\"):\n    if(mode == \"tissue\"):\n        title = \"Tissue Segmentation\"\n        mask_files = tissue_mask_png_files\n        class_id_map = tissue_class_id_map\n        colormap = tissue_colormap\n    elif(mode == \"nuclei\"):\n        title = \"Nuclei Segmentation\"\n        mask_files = nuclei_mask_png_files\n        class_id_map = nuclei_class_id_map\n        colormap = nuclei_colormap\n    else:\n        raise ValueError(\"mode must be 'nuclei' or 'tissue'\")\n\n    NUM_CLASSES = len(class_id_map)\n    \n    # Compute class weights\n    print(\"🔹 Computing class weights...\")\n    all_masks = [load_mask(m) for m in mask_files]\n    flat_masks = np.argmax(np.stack(all_masks, axis=0), axis=-1).flatten()\n    class_counts = np.bincount(flat_masks, minlength=NUM_CLASSES).astype(np.float32)\n    class_counts[class_counts == 0] = 1e-6  # avoid div/0\n    class_weights = flat_masks.shape[0] / (NUM_CLASSES * class_counts)\n    class_weights = class_weights / np.mean(class_weights)\n    class_weights = tf.constant(class_weights, dtype=tf.float32)\n    # print(f\"Class Weights: {class_weights.numpy()}\")\n\n    # Train/val split\n    img_train, img_val, mask_train, mask_val = train_test_split(\n        img_files, mask_files, test_size=0.2, random_state=42\n    )\n\n    # Build model\n    model = build_segmentation_model(model_type, (IMG_SIZE, IMG_SIZE, 3), NUM_CLASSES)\n    optimizer = tf.keras.optimizers.Adam(LR)\n    loss_fn = lambda y_true, y_pred: combined_weighted_dice_loss(\n        y_true, y_pred, class_weights=class_weights, alpha=0.7, gamma=2.0\n    )\n\n    metrics = [dice_coef, iou_coef,\"accuracy\"]\n\n    \n    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n    \n    # Generators\n    augment = get_augmentation()\n    train_gen = data_generator(img_train, mode=mode, mask_files=mask_train, batch_size=BATCH_SIZE, augment=None)\n    val_gen   = data_generator(img_val, mode=mode, mask_files=mask_val, batch_size=BATCH_SIZE, augment=None)\n    train_steps = len(img_train) // BATCH_SIZE\n    val_steps   = len(img_val) // BATCH_SIZE\n\n    # Callbacks\n    checkpoint = ModelCheckpoint(\n        f\"{title}_best_model.h5\", monitor=\"val_dice_coef\", mode=\"max\", save_best_only=True\n    )\n    lr_scheduler = ReduceLROnPlateau(\n        monitor=\"val_loss\", factor=0.3, patience=10, min_lr=1e-6\n    )\n    display_cb = DisplayPredictionsCallback(\n        [load_image(p) for p in img_val[:10]],\n        [load_mask(p) for p in mask_val[:10]],\n        mode=mode, interval=10\n    )\n\n    # Train\n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        steps_per_epoch=train_steps,\n        validation_steps=val_steps,\n        epochs=EPOCHS,\n        callbacks=[checkpoint, display_cb, lr_scheduler],\n        verbose=1\n    )\n\n    # Visualize random validation sample\n    idx = np.random.randint(0, len(img_val))\n    x_val = load_image(img_val[idx])\n    mask_rgb = load_mask(mask_val[idx])\n    y_val = preprocess_mask(mask_rgb, mode=mode)\n    true_mask = np.argmax(y_val, axis=-1)\n    pred_mask = np.argmax(model.predict(np.expand_dims(x_val, 0))[0], axis=-1)\n\n    true_rgb = mask_to_color(true_mask, colormap, class_id_map)  # CHANGED\n    pred_rgb = mask_to_color(pred_mask, colormap, class_id_map)  # CHANGED\n    \n    plt.figure(figsize=(15,5))\n    plt.subplot(1,3,1); plt.imshow(x_val); plt.title(\"Input\"); plt.axis(\"off\")\n    plt.subplot(1,3,2); plt.imshow(true_rgb); plt.title(\"Ground Truth\"); plt.axis(\"off\")\n    plt.subplot(1,3,3); plt.imshow(pred_rgb); plt.title(\"Prediction\"); plt.axis(\"off\")\n    plt.show()\n\n    return model, history, img_val, mask_val\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:23.711851Z","iopub.execute_input":"2025-10-01T18:11:23.712044Z","iopub.status.idle":"2025-10-01T18:11:23.945764Z","shell.execute_reply.started":"2025-10-01T18:11:23.712028Z","shell.execute_reply":"2025-10-01T18:11:23.945194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization Function","metadata":{}},{"cell_type":"code","source":"def visualize_predictions(X, Y_true, Y_pred, idx, class_id_map, colormap, title_prefix=\"\"):\n    \"\"\"\n    Show input image, GT mask, predicted mask\n    X: images (B, H, W, 3)\n    Y_true: one-hot masks (B, H, W, C)\n    Y_pred: probability masks (B, H, W, C)\n    idx: index to visualize\n    \"\"\"\n    img = X[idx]\n\n    plt.figure(figsize=(15, 5))\n\n    plt.subplot(1, 3, 1)\n    plt.title(\"🖼 Input Image\")\n    plt.imshow(img)\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 2)\n    plt.title(\"🎯 Ground Truth\")\n    plt.imshow(Y_true[idx])\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 3)\n    plt.title(\"🔮 Predicted Mask\")\n    plt.imshow(Y_pred[idx])\n    plt.axis(\"off\")\n\n    plt.suptitle(f\"{title_prefix} Sample #{idx}\", fontsize=14)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:28.919880Z","iopub.execute_input":"2025-10-01T18:11:28.920456Z","iopub.status.idle":"2025-10-01T18:11:28.926194Z","shell.execute_reply.started":"2025-10-01T18:11:28.920426Z","shell.execute_reply":"2025-10-01T18:11:28.925406Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training History","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_training_history(history, title=\"Training History\"):\n    \"\"\"\n    Plot loss and metrics from Keras history object.\n    \n    history: history object returned by model.fit()\n    title: plot title\n    \"\"\"\n    # Extract metrics\n    loss = history.history['loss']\n    val_loss = history.history.get('val_loss', None)\n    \n    acc = history.history.get('accuracy', None)\n    val_acc = history.history.get('val_accuracy', None)\n    \n    dice = history.history.get('dice_coef', None)\n    val_dice = history.history.get('val_dice_coef', None)\n\n    epochs = range(1, len(loss) + 1)\n\n    plt.figure(figsize=(16, 5))\n\n    # 1. Loss\n    plt.subplot(1, 3, 1)\n    plt.plot(epochs, loss, 'b-', label='Train Loss')\n    if val_loss is not None:\n        plt.plot(epochs, val_loss, 'r--', label='Val Loss')\n    plt.title(f'{title} - Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # 2. Accuracy\n    if acc is not None:\n        plt.subplot(1, 3, 2)\n        plt.plot(epochs, acc, 'b-', label='Train Accuracy')\n        if val_acc is not None:\n            plt.plot(epochs, val_acc, 'r--', label='Val Accuracy')\n        plt.title(f'{title} - Accuracy')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.legend()\n\n    # 3. Dice coefficient\n    if dice is not None:\n        plt.subplot(1, 3, 3)\n        plt.plot(epochs, dice, 'b-', label='Train Dice')\n        if val_dice is not None:\n            plt.plot(epochs, val_dice, 'r--', label='Val Dice')\n        plt.title(f'{title} - Dice Coefficient')\n        plt.xlabel('Epoch')\n        plt.ylabel('Dice')\n        plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:31.423647Z","iopub.execute_input":"2025-10-01T18:11:31.423920Z","iopub.status.idle":"2025-10-01T18:11:31.431134Z","shell.execute_reply.started":"2025-10-01T18:11:31.423897Z","shell.execute_reply":"2025-10-01T18:11:31.430528Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# History to Table","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef history_to_table(history, round_digits=4):\n    \"\"\"\n    Converts Keras training history into a pandas DataFrame for reporting.\n    \n    Parameters:\n        history: History object returned by model.fit()\n        round_digits: Number of decimal places to round the metrics\n        \n    Returns:\n        df: pandas DataFrame with metrics per epoch\n    \"\"\"\n    # Extract history dictionary\n    hist_dict = history.history\n    \n    # Create DataFrame\n    df = pd.DataFrame(hist_dict)\n    \n    # Optionally round values for easier presentation\n    df = df.round(round_digits)\n    \n    # Add epoch numbers\n    df.insert(0, \"Epoch\", range(1, len(df)+1))\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:35.608621Z","iopub.execute_input":"2025-10-01T18:11:35.608894Z","iopub.status.idle":"2025-10-01T18:11:35.613640Z","shell.execute_reply.started":"2025-10-01T18:11:35.608873Z","shell.execute_reply":"2025-10-01T18:11:35.612737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Per class Metrics","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef evaluate_per_class(model, mode=\"nuclei\"):\n    \"\"\"\n    Computes per-class Dice, IoU, and Recall for the validation set.\n    \n    Parameters:\n        model: trained Keras model\n        mode: \"nuclei\" or \"tissue\"\n        \n    Returns:\n        result_df: pandas DataFrame with per-class metrics\n    \"\"\"\n    # --- Choose files & class maps ---\n    if mode == \"nuclei\":\n        class_id_map = nuclei_class_id_map\n        X_val_files  = roi_tif_files\n        Y_val_files  = nuclei_mask_png_files\n    elif mode == \"tissue\":\n        class_id_map = tissue_class_id_map\n        X_val_files  = roi_tif_files\n        Y_val_files  = tissue_mask_png_files\n    else:\n        raise ValueError(\"mode must be 'nuclei' or 'tissue'\")\n\n    NUM_CLASSES = len(class_id_map)\n\n    # --- Load data into arrays ---\n    X_val = np.array([load_image(p) for p in X_val_files])\n    Y_val = np.array([preprocess_mask(load_mask(p), mode=mode) for p in Y_val_files])\n\n    # --- Predictions ---\n    Y_pred = model.predict(X_val, batch_size=8)\n    Y_pred = np.argmax(Y_pred, axis=-1).flatten()\n    Y_true = np.argmax(Y_val, axis=-1).flatten()\n\n    # --- Per-class metrics ---\n    metrics = []\n    for cls_name, cls_idx in sorted(class_id_map.items(), key=lambda x: x[1]):\n        y_true_cls = (Y_true == cls_idx).astype(np.uint8)\n        y_pred_cls = (Y_pred == cls_idx).astype(np.uint8)\n\n        intersection = np.sum(y_true_cls * y_pred_cls)\n        union = np.sum(y_true_cls) + np.sum(y_pred_cls)\n        dice = (2. * intersection) / union if union > 0 else np.nan\n\n        union_iou = np.sum(y_true_cls | y_pred_cls)\n        iou = intersection / union_iou if union_iou > 0 else np.nan\n\n        recall = np.sum(y_true_cls & y_pred_cls) / np.sum(y_true_cls) if np.sum(y_true_cls) > 0 else np.nan\n\n        metrics.append({\n            \"Class\": cls_name,\n            \"Dice\": round(dice, 4) if not np.isnan(dice) else None,\n            \"IoU\": round(iou, 4) if not np.isnan(iou) else None,\n            \"Recall\": round(recall, 4) if not np.isnan(recall) else None\n        })\n\n    return pd.DataFrame(metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:27:19.192835Z","iopub.execute_input":"2025-10-01T18:27:19.193468Z","iopub.status.idle":"2025-10-01T18:27:19.201701Z","shell.execute_reply.started":"2025-10-01T18:27:19.193442Z","shell.execute_reply":"2025-10-01T18:27:19.200928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Compile and Train","metadata":{}},{"cell_type":"code","source":"MODEL_TYPE = \"multires\"     # unet / attention / sharp / multires\nCURRENT_SUBJECT = \"tissue\"    # tissue / nuclei\n\nmodel, history, X_val, Y_val = train_segmentation(\n    roi_tif_files,\n    model_type=MODEL_TYPE,\n    mode = CURRENT_SUBJECT\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:11:41.929546Z","iopub.execute_input":"2025-10-01T18:11:41.930227Z","iopub.status.idle":"2025-10-01T18:21:05.493238Z","shell.execute_reply.started":"2025-10-01T18:11:41.930206Z","shell.execute_reply":"2025-10-01T18:21:05.492545Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training History","metadata":{}},{"cell_type":"code","source":"plot_training_history(history)\nhistory_to_table(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:36.525376Z","iopub.execute_input":"2025-10-01T18:21:36.526098Z","iopub.status.idle":"2025-10-01T18:21:37.130327Z","shell.execute_reply.started":"2025-10-01T18:21:36.526074Z","shell.execute_reply":"2025-10-01T18:21:37.129519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Per Class Data","metadata":{}},{"cell_type":"code","source":"per_class_results = evaluate_per_class(model, CURRENT_SUBJECT)\nprint(per_class_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:27:42.226284Z","iopub.execute_input":"2025-10-01T18:27:42.226848Z","iopub.status.idle":"2025-10-01T18:30:14.640420Z","shell.execute_reply.started":"2025-10-01T18:27:42.226821Z","shell.execute_reply":"2025-10-01T18:30:14.639589Z"}},"outputs":[],"execution_count":null}]}